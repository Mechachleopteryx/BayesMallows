#' Compute the posterior distribution for the Mallows model.
#'
#' @param rankings A matrix of ranked items. See \code{\link{create_ranking}} if
#'   you have an ordered set of items that needs to be converted to rankings. If
#'   \code{preferences} is provided, \code{rankings} is an optional initial
#'   value of the rankings, generated by \code{\link{generate_initial_ranking}}.
#' @param preferences A dataframe with the transitive closure of pairwise
#'   comparisons. If it has class \code{BayesMallowsTC}, generated by
#'   \code{\link{generate_transitive_closure}}, this will be used. Otherwise,
#'   \code{\link{generate_transitive_closure}} will be called on this dataframe.
#' @param metric The distance metric to use. Currently available ones are
#'   \code{"footrule"}, \code{"spearman"}, \code{"kendall"}, \code{"cayley"},
#'   and \code{"hamming"}.
#' @param n_clusters Number of clusters. Defaults to 1, which means no
#'   clustering.
#' @param include_wcd Logical, defaults to \code{TRUE} if \code{n_clusters > 1},
#'   otherwise \code{FALSE}.
#' @param lambda Parameter for the prior distribution of \code{alpha}. Defaults
#'   to 0.1.
#' @param psi Hyperparameter for the Dirichlet prior distribution used in clustering.
#' @param nmc Number of Monte Carlo samples to keep.
#' @param leap_size Step size of the leap-and-shift proposal distribution.
#'   Defaults to NULL, which means that it is automatically set to n/5.
#' @param sd_alpha Standard deviation of the proposal distribution for alpha.
#' @param alpha_init Initial value of alpha.
#' @param alpha_jump How many times should we sample \code{rho} between each
#'   time we sample \code{alpha}. Setting \code{alpha_jump} to a high number can
#'   significantly speed up computation time, since we then do not have to do
#'   expensive computation of the partition function.
#' @param thinning Keep every \code{thinning} iteration of \code{rho}.
#' @param aug_diag_thinning The interval in which we save augmentation
#'   diagnostics.
#' @param save_augmented_data Whether or not to save the augmented data every
#'   \code{thinning}th iteration, for the case of missing data or pairwise
#'   preferences.
#' @param is_fit Importance sampling estimate of the partition function, computed
#'   with \code{\link{compute_importance_sampling_estimate}}.
#'
#' @return A list of class BayesMallows.
#' @references \insertRef{vitelli2018}{BayesMallows}
#' @seealso \code{\link{assess_convergence}}, \code{\link{plot.BayesMallows}}.
#' @export
#' @importFrom rlang .data
compute_mallows <- function(rankings = NULL,
                            preferences = NULL,
                            metric = "footrule",
                            n_clusters = 1,
                            include_wcd = (n_clusters > 1),
                            lambda = 0.1,
                            psi = 10,
                            nmc = 2000,
                            leap_size = NULL,
                            sd_alpha = 0.1,
                            alpha_init = 1,
                            alpha_jump = 1,
                            thinning = 1,
                            aug_diag_thinning = 100,
                            save_augmented_data = FALSE,
                            is_fit = NULL
                            ){

  # Check that at most one of rankings and preferences is set
  stopifnot(!is.null(rankings) || !is.null(preferences))


  # Deal with pairwise comparisons. Generate rankings compatible with them.
  if(!is.null(preferences)){

    if(!("BayesMallowsTC" %in% class(preferences))){
      preferences <- generate_transitive_closure(preferences)
    }

    # Find all the constrained elements per assessor
    constrained <- dplyr::group_by(preferences, .data$assessor)
    constrained <- dplyr::summarise(constrained,
                                    items = list(unique(c(.data$bottom_item, .data$top_item))))
    constrained <- tidyr::unnest(constrained)
    constrained <- as.matrix(constrained)

    preferences <- as.matrix(preferences)

    if(is.null(rankings)){
      rankings <- generate_initial_ranking(preferences)
    }
  } else {
    constrained <- NULL
  }

  # If there are no missing values nor preference, increase aug_diag_thinning
  if(is.null(preferences) && sum(is.na(rankings)) == 0){
    aug_diag_thinning <- nmc
  }

  # Check that all rows of rankings are proper permutations
  if(!all(apply(rankings, 1, validate_permutation))){
    stop("Not valid permutation.")
  }

  # Check that we do not jump over all alphas
  stopifnot(alpha_jump < nmc)

  # Check that we do not jump over all rhos
  stopifnot(thinning < nmc)

  # Find the number of items
  n_items <- ncol(rankings)

  # Set leap_size if it is not alredy set.
  if(is.null(leap_size)) leap_size <- floor(n_items / 5)

  # Extract the right sequence of cardinalities, if relevant
  if(!is.null(is_fit)){
    cardinalities <- NULL
    message("Using user-provided importance sampling estimate of partition function.")
  } else if(metric %in% c("footrule", "spearman")){
    # Extract the relevant rows from partition_function_data
    # Note that we need to evaluate the right-hand side, in particular metric,
    # to avoid confusion with columns of the tibble
    relevant_params <- dplyr::filter(partition_function_data,
                                     .data$n_items == !!n_items,
                                     .data$metric == !!metric
    )

    type <- dplyr::pull(relevant_params, type)

    if(type == "cardinalities") {
      cardinalities <- unlist(relevant_params$values)
      is_fit <- NULL
    } else if(type == "importance_sampling"){
      cardinalities <- NULL
      is_fit <- unlist(relevant_params$values)
    } else {
      stop("Precomputed partition function not available yet. Consider computing one
           with the function compute_importance_sampling_estimate(), and provide it
           in the is_fit argument to compute_mallows().")
    }

  } else if (metric %in% c("cayley", "hamming", "kendall")) {
    cardinalities <- NULL
    is_fit <- NULL
  } else {
    stop(paste("Unknown metric", metric))
  }



  # Transpose rankings to get samples along columns, since we typically want
  # to extract one sample at a time. armadillo is column major, just like rankings
  fit <- run_mcmc(rankings = t(rankings),
                  nmc = nmc,
                  preferences = preferences,
                  constrained = constrained,
                  cardinalities = cardinalities,
                  is_fit = is_fit,
                  metric = metric,
                  n_clusters = n_clusters,
                  include_wcd = include_wcd,
                  lambda = lambda,
                  leap_size = leap_size,
                  sd_alpha = sd_alpha,
                  alpha_init = alpha_init,
                  alpha_jump = alpha_jump,
                  thinning = thinning,
                  aug_diag_thinning = aug_diag_thinning,
                  save_augmented_data = save_augmented_data
                  )

  # Add some arguments
  fit$metric <- metric
  fit$lambda <- lambda
  fit$nmc <- nmc
  fit$n_items <- n_items
  fit$n_clusters <- n_clusters
  fit$alpha_jump <- alpha_jump
  fit$thinning <- thinning
  fit$leap_size <- leap_size
  fit$sd_alpha <- sd_alpha
  fit$aug_diag_thinning <- aug_diag_thinning
  fit$include_wcd <- include_wcd
  fit$save_augmented_data <- save_augmented_data

  # If no data augmentation has happened, do not include aug_acceptance
  # Otherwise, convert to fraction
  if(!fit$any_missing && !fit$augpair) {
    fit$aug_acceptance <- NULL
    fit$aug_diag_thinning <- NULL
  } else {
    fit$aug_acceptance <- fit$aug_acceptance / aug_diag_thinning
  }

  # Add names of item
  if(!is.null(colnames(rankings))) {
    rownames(fit$rho) <- colnames(rankings)
  } else {
    rownames(fit$rho) <- paste("Item", seq(from = 1, to = nrow(fit$rho), by = 1))
  }

  fit$items <- rownames(fit$rho)

  # Tidy MCMC results
  fit <- tidy_mcmc(fit)


  # Add class attribute
  class(fit) <- "BayesMallows"

  return(fit)

}
