---
title: "Introduction to BayesMallows"
author: "Oystein Sorensen"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
set.seed(232312)
```

## Introduction

The `BayesMallows` package implements the Bayesian methods for analysis of rank data described in @vitelli2018. This vignette describes the basic usage of the function for complete data. Other vignettes will describe incomplete data and clustering.

We start by loading the package.
```{r}
library(BayesMallows)
```


## Potato Data

The `BayesMallows` package comes with example data described in @liu2018. A total of 12 assessors were asked to rank 20 potatoes based on their weight. In the round, the assessors were only allowed to study the potatoes visually, while in the second round, the assessors were also allowed to hold the potatoes in their hands in order to compare them. The data sets are named `potato_visual` and `potato_weighing`, respectively. The true ordering of the potatoes' weights are stored in the vector `potato_true_ranking`.

The `potato_visual` dataset is shown below. The column names P1, ..., P20 represent potatoes, and the row names A1, ..., A12 represent assessors. The `potato_weighing` dataset has a similar structure.

```{r, echo=FALSE, results='asis'}
knitr::kable(potato_visual, caption = "Example dataset potato_visual.")
```


## Algorithm Tuning

The `compute_mallows` function is the workhorse of `BayesMallows`, and finds the posterior distribution of the scale parameter $\alpha$ and the latent ranks $\rho$ of the Bayesian Mallows' rank model. To see all its arguments, please run `?compute_mallows` in the console.

We start by using all the default values of the parameters, so we only need to supply the matrix of ranked items. We use the `potato_visual` data printed above.

```{r}
model_fit <- compute_mallows(potato_visual)
```

The argument returned is a list object of class `BayesMallows`, which contains a whole lot of information about the MCMC run.

```{r}
str(model_fit)
```

The function `assess_convergence` takes an object of class `BayesMallows` and produces plots for visual convergence assessment. We start by studing $\alpha$, which is the default. The plot is shown below, and looks good enough, at least to begin with.

```{r, fig.width=6, fig.align='center'}
assess_convergence(model_fit)
```

Next, we study the convergence of $\rho$. To avoid too complicated plots, we pick 5 items to plot.

```{r, fig.width=6, fig.align='center'}
assess_convergence(model_fit, type = "rho", items = 1:5)
```

Based on these plots, it looks like the algorithm starts to converge after around 1000 iterations. Discarding the first 2000 iterations as burn-in hence seems like a safe choice.

## Studying the Posterior Distributions

Once we are confident that the algorithm parameters are reasonable, we can study the posterior distributions of the model parameters using the generic function `plot.BayesMallows`.

### Scale Parameter

With a burnin of 2000, the original `model_fit` object has only 1000 MCMC samples. We compare this to a new object, which has 100 times as many samples. We see that the densities have the same mean, but that with a larger number of iterations, the density is more symmetric and less bumpy.

```{r, fig.show='hold'}
model_fit_big <- compute_mallows(potato_visual, nmc = 1e5 + 2000)
plot(model_fit, burnin = 2000)
plot(model_fit_big, burnin = 2000)
```


The object `model_fit_big` contain a fair amount of data, so we tidy up before going on:
```{r}
rm(model_fit_big)
```


### Latent Ranks

Obtaining posterior samples from $\rho$ is in general harder than for $\alpha$. Some items tend to be very sticky. 

We start by plotting the `model_fit` object from above, with 3000 iterations, discarding the first 2000 as burn-in.

```{r, fig.width=6, fig.height=6, fig.align='center'}
plot(model_fit, burnin = 2000, type = "rho", items = 1:20)
```

#### Jumping over Alphas
We try again with more samples. To avoid updating $\alpha$ in each step, we set `alpha_jump = 100`. This implies that $\alpha$ is updated and saved only every 10th iteration on $\rho$. Before computing the posterior distribution with this parameter, we do a new convergence assessment:

```{r}
model_fit <- compute_mallows(potato_visual, nmc = 10000, alpha_jump = 10)
```

The trace indicates convergence after around 200 iterations of $\alpha$, i.e., after 2000 Monte Carlo samples. 

```{r, fig.width=6, fig.align='center'}
assess_convergence(model_fit, type = "alpha")
```

The convergence plot for $\rho$ agrees that the MCMC algorithms seems to have converged after 2000 iterations.

```{r, fig.width=6, fig.align='center'}
assess_convergence(model_fit, type = "rho", items = 1:5)
```

We now run the algorithm with a 1e5 iterations of $\rho$, sampling $\alpha$ only every 10th step:

```{r}
model_fit <- compute_mallows(potato_visual, nmc = 1e5 + 2000, alpha_jump = 10)
```

The posterior density of $\alpha$ looks very similar to what it did above:
```{r, fig.align='center'}
plot(model_fit, burnin = 2000)
```

```{r, fig.width=6, fig.height=6, fig.align='center'}
plot(model_fit, burnin = 2000, type = "rho", items = 1:20)
```

Now at least there is some more variation. We can compare the posterior probabilities to the average rank obtained by each potato. There is an overall agreement between the mean rankings in the data, which is reassuring.

```{r, results='asis', message=FALSE}
library(dplyr)
library(tidyr)
as_tibble(potato_visual) %>% 
  select(P1:P6) %>% 
  gather(key = "Potato", value = "Rank") %>% 
  group_by(Potato) %>% 
  summarise(
    Mean = mean(Rank),
    StDev = sd(Rank)
  ) %>% 
  knitr::kable(digits = 1)
```

#### Thinning
Saving a large number of iterations of $\rho$ starts to get quite expensive, so to this end, we have a `thinning` parameter, which specifies that only each `thinning`th iteration of $\rho$ should be saved to memory. We increase the number of iterations tenfold, while setting `thinning = 10`.

```{r}
model_fit <- compute_mallows(potato_visual, nmc = 1e6 + 2000, 
                             alpha_jump = 10, thinning = 10)
```

```{r, fig.width=6, fig.height=6, fig.align='center'}
plot(model_fit, burnin = 2000, type = "rho", items = 1:20)
```

## Varying the Distance Metric
We can try to use the Kendall distance instead of the footrule distance.

```{r}
model_fit <- compute_mallows(potato_visual, metric = "kendall",
                             nmc = 1e5 + 2000, alpha_jump = 10)
```

```{r, fig.width=6, fig.height=6, fig.align='center'}
plot(model_fit, burnin = 2000, type = "rho", items = 1:20)
```

And we can use Spearman distance. In this case, since the number of potatoes (20) is large than the maximum number for which we have an exact computation of the partition function (14), importance sampling estimates are used. This is handled automatically by `compute_mallows`. Note that the posterior ranks are less peaked with Spearman distance. This agrees with the results seen in @liu2018.

```{r}
model_fit <- compute_mallows(potato_visual, metric = "spearman",
                             nmc = 1e5 + 2000, alpha_jump = 10)
```

```{r, fig.width=6, fig.height=6, fig.align='center'}
plot(model_fit, burnin = 2000, type = "rho", items = 1:20)
```


# References
